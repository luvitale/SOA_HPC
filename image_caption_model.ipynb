{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_caption_model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMhmVzhS8dXsYMt9hbfWy5C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luvitale/SOA_HPC/blob/main/image_caption_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-yDBWa5eM3i"
      },
      "source": [
        "# Introducción\n",
        "\n",
        "La computación de alto rendimiento (HPC) representa la capacidad de procesar datos y realizar cálculos complejos a velocidades muy altas.\n",
        "\n",
        "Es a través de los datos que se hacen descubrimientos científicos innovadores, se alimentan las innovaciones que cambian los juegos y se mejora la calidad de vida de miles de millones de personas en todo el mundo. HPC es la base de los avances científicos, industriales y sociales.\n",
        "\n",
        "Un subconjunto de la inteligencia artificial (IA), el aprendizaje automático (ML) es el área de la ciencia computacional que se centra en el análisis y la interpretación de patrones y estructuras de datos que hacen posible el aprendizaje, el razonamiento y la toma de decisiones sin interacción humana.\n",
        "\n",
        "El Aprendizaje Automático (AA) consta de tres partes:\n",
        "\n",
        "El algoritmo computacional, situado en el núcleo de la toma de determinaciones.\n",
        "Las variables y las funciones que conforman la decisión.\n",
        "El conocimiento base según el cual se sabe la respuesta que permite aprender al sistema (lo entrena).\n",
        "TensorFlow es una plataforma de código abierto de extremo a extremo para el aprendizaje automático. Cuenta con un ecosistema integral y flexible de herramientas, bibliotecas y recursos de la comunidad que les permite a los investigadores innovar con el aprendizaje automático y, a los desarrolladores, compilar e implementar con facilidad aplicaciones con tecnología de AA.\n",
        "\n",
        "Keras es una API diseñada para seres humanos, no para máquinas. Keras sigue las mejores prácticas para reducir la carga cognitiva: ofrece API consistentes y simples, minimiza la cantidad de acciones del usuario necesarias para casos de uso comunes y proporciona mensajes de error claros y procesables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWSs3OjawLhS"
      },
      "source": [
        "## Armado del ambiente\n",
        "<!---\n",
        "En esta sección deben estar todos los comandos\n",
        "previos, que son necesarios para la ejecución del ejercicio. \n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pBhxUJzwRu9"
      },
      "source": [
        "### Instalación de TensorFlow con GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHZlOA0vwW8k",
        "outputId": "7ad72854-1a01-47b2-ad76-a14306999f1a"
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.4.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.19.5)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.5.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.34.1)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.17.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.7.4.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.5.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.36.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (1.32.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (0.4.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (57.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu) (3.3.4)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow-gpu) (1.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow-gpu) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow-gpu) (3.5.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCL_5v3EwX8i"
      },
      "source": [
        "### Verificación de la versión instalada de TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpu2Yf_jweaN",
        "outputId": "491bedc4-6cba-4dcc-9bf7-3167dcfa17b5"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta2H5bwlwll4"
      },
      "source": [
        "### Configuración"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yukNtYrywmwK"
      },
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import requests\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.applications import efficientnet\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "seed = 111\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXg9X73xw4zx"
      },
      "source": [
        "### Descargando el conjunto de datos\n",
        "\n",
        "Se utilizan un conjunto de datos Flickr8K, el cual comprende más de 8 mil imágenes, cada una de las cuales está emparejada con cinco subtítulos diferentes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANbGVlOXxMvh",
        "outputId": "fb62c079-3345-466f-b7a7-d43bb8f64738"
      },
      "source": [
        "!wget -nc https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip -O dataset.zip\n",
        "!wget -nc https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip -O text.zip\n",
        "!unzip -n dataset.zip\n",
        "!unzip -n text.zip\n",
        "!rm dataset.zip text.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-19 09:02:53--  https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/124585957/47f52b80-3501-11e9-8f49-4515a2a3339b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210719%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210719T090253Z&X-Amz-Expires=300&X-Amz-Signature=e05f486222f115775fb9ce9960bf4e22232b2bebe05744ac4f8f9d10d1448de5&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=124585957&response-content-disposition=attachment%3B%20filename%3DFlickr8k_Dataset.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-07-19 09:02:53--  https://github-releases.githubusercontent.com/124585957/47f52b80-3501-11e9-8f49-4515a2a3339b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210719%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210719T090253Z&X-Amz-Expires=300&X-Amz-Signature=e05f486222f115775fb9ce9960bf4e22232b2bebe05744ac4f8f9d10d1448de5&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=124585957&response-content-disposition=attachment%3B%20filename%3DFlickr8k_Dataset.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.108.154, 185.199.109.154, 185.199.110.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.108.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115419746 (1.0G) [application/octet-stream]\n",
            "Saving to: ‘dataset.zip’\n",
            "\n",
            "dataset.zip         100%[===================>]   1.04G  99.1MB/s    in 11s     \n",
            "\n",
            "2021-07-19 09:03:04 (95.7 MB/s) - ‘dataset.zip’ saved [1115419746/1115419746]\n",
            "\n",
            "--2021-07-19 09:03:05--  https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/124585957/47f52b80-3501-11e9-8d2e-dd69a21a4362?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210719%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210719T090305Z&X-Amz-Expires=300&X-Amz-Signature=046128f6465c0f69434007578090d0ae552d2bb003df16d43c251879d4e53715&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=124585957&response-content-disposition=attachment%3B%20filename%3DFlickr8k_text.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-07-19 09:03:05--  https://github-releases.githubusercontent.com/124585957/47f52b80-3501-11e9-8d2e-dd69a21a4362?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210719%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210719T090305Z&X-Amz-Expires=300&X-Amz-Signature=046128f6465c0f69434007578090d0ae552d2bb003df16d43c251879d4e53715&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=124585957&response-content-disposition=attachment%3B%20filename%3DFlickr8k_text.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.108.154, 185.199.109.154, 185.199.110.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.108.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2340801 (2.2M) [application/octet-stream]\n",
            "Saving to: ‘text.zip’\n",
            "\n",
            "text.zip            100%[===================>]   2.23M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2021-07-19 09:03:05 (33.2 MB/s) - ‘text.zip’ saved [2340801/2340801]\n",
            "\n",
            "Archive:  dataset.zip\n",
            "Archive:  text.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI249ol9xkRH"
      },
      "source": [
        "### Parámetros\n",
        "\n",
        "El parámetro de entrenamiento **EPOCHS** es el más importante, ya que un número bajo genererá menos procesamiento pero es mayor el margen de error al asignarle un título a las imágenes. \n",
        "\n",
        "A la inversa, un número alto generará más procesamiento (siendo más lento) pero es menor el margen de error al asignarle un título a las imágenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQKTju0Xx4DK"
      },
      "source": [
        "# Ruta de las imágenes\n",
        "IMAGES_PATH = \"Flicker8k_Dataset\"\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Dimensión de las imágenes\n",
        "IMAGE_SIZE_X = 100 #@param {type: \"slider\", min: 50, max: 500}\n",
        "IMAGE_SIZE_Y = 100 #@param {type: \"slider\", min: 50, max: 500}\n",
        "IMAGE_SIZE = (IMAGE_SIZE_X, IMAGE_SIZE_Y)\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Tamaño de vocabulario\n",
        "VOCAB_SIZE = 500 #@param {type: \"slider\", min: 100, max: 20000}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Longitud fija permitida para cualquier secuencia\n",
        "SEQ_LENGTH = 20 #@param {type: \"slider\", min: 10, max: 1000}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Dimensión de las incrustaciones de imagen\n",
        "EMBED_DIM = 8 #@param {type: \"slider\", min: 2, max: 1024}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Número de cabezas de auto-atención\n",
        "NUM_HEADS = 2 #@param {type: \"slider\", min: 1, max: 20}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Unidades por capa en la red de alimentación hacia adelante\n",
        "FF_DIM = 256 #@param {type: \"slider\", min: 32, max: 1024}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Otros parámetros de entrenamiento\n",
        "BATCH_SIZE = 8 #@param {type: \"slider\", min: 2, max: 1024}\n",
        "EPOCHS = 5 #@param {type: \"slider\", min: 1, max: 100}\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "#@markdown --- "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-we-jb0ZyC67"
      },
      "source": [
        "### Preparando el conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txZ9amZ_yF7x",
        "outputId": "985c464c-f83d-4bf0-9219-a77eaf16cc28"
      },
      "source": [
        "def load_captions_data(filename):\n",
        "    \"\"\"Carga de datos de subtítulos (texto) y los asigna a las imágenes correspondientes.\n",
        "\n",
        "    Args:\n",
        "        filename: Ruta del archivo de texto que contiene datos de subtítulos.\n",
        "\n",
        "    Returns:\n",
        "        caption_mapping: Nombre de imágenes de mapas de diccionario y subtítulos correspondientes\n",
        "        text_data: Lista que contiene todos los subtítulos disponibles\n",
        "    \"\"\"\n",
        "\n",
        "    with open(filename) as caption_file:\n",
        "        caption_data = caption_file.readlines()\n",
        "        caption_mapping = {}\n",
        "        text_data = []\n",
        "\n",
        "        for line in caption_data:\n",
        "            line = line.rstrip(\"\\n\")\n",
        "            # El nombre de la imagen y los subtítulos se separan mediante un tab\n",
        "            img_name, caption = line.split(\"\\t\")\n",
        "            # Cada imagen se repite cinco veces para los cinco subtítulos diferentes\n",
        "            # El nombre de la imagen tiene un prefijo `#(caption_number)`\n",
        "            img_name = img_name.split(\"#\")[0]\n",
        "            img_name = os.path.join(IMAGES_PATH, img_name.strip())\n",
        "\n",
        "            if img_name.endswith(\"jpg\"):\n",
        "                # Se añade un comienzo <start> y un fin <end> a cada subtítulo\n",
        "                caption = \"<start> \" + caption.strip() + \" <end>\"\n",
        "                text_data.append(caption)\n",
        "\n",
        "                if img_name in caption_mapping:\n",
        "                    caption_mapping[img_name].append(caption)\n",
        "                else:\n",
        "                    caption_mapping[img_name] = [caption]\n",
        "\n",
        "        return caption_mapping, text_data\n",
        "\n",
        "\n",
        "def train_val_split(caption_data, train_size=0.8, shuffle=True):\n",
        "    \"\"\"Se divide el conjunto de datos de subtitulos en conjuntos de tren y validación.\n",
        "\n",
        "    Args:\n",
        "        caption_data (dict): Diccionario que contiene los datos de los subtítulos asignados\n",
        "        train_size (float): Fracción de todo el conjunto de datos completo para usar como datos de entrenamiento\n",
        "        shuffle (bool): Si se debe mezclar el conjunto antes de dividir\n",
        "\n",
        "    Returns:\n",
        "        Conjunto de datos de preparación y validación como dos dictados separados\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Obtener la lista de todos los nombres de imágenes\n",
        "    all_images = list(caption_data.keys())\n",
        "\n",
        "    # 2. Mezclar si es necesario\n",
        "    if shuffle:\n",
        "        np.random.shuffle(all_images)\n",
        "\n",
        "    # 3. Dividir en conjuntos de capacitación y validación\n",
        "    train_size = int(len(caption_data) * train_size)\n",
        "\n",
        "    training_data = {\n",
        "        img_name: caption_data[img_name] for img_name in all_images[:train_size]\n",
        "    }\n",
        "    validation_data = {\n",
        "        img_name: caption_data[img_name] for img_name in all_images[train_size:]\n",
        "    }\n",
        "\n",
        "    # 4. Devuelve las divisiones \n",
        "    return training_data, validation_data\n",
        "\n",
        "\n",
        "# Cargar el conjunto de datos\n",
        "captions_mapping, text_data = load_captions_data(\"Flickr8k.token.txt\")\n",
        "\n",
        "# Dividir el conjunto de datos en entrenamiento y validaciones\n",
        "train_data, valid_data = train_val_split(captions_mapping)\n",
        "print(\"Número de muestras de entrenamiento: \", len(train_data))\n",
        "print(\"Número de muestras de validación: \", len(valid_data))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Número de muestras de entrenamiento:  6472\n",
            "Número de muestras de validación:  1619\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9dOZwkj1Mke"
      },
      "source": [
        "### Vectorizar los datos de texto\n",
        "Se utiliza la TextVectorization capa para vectorizar los datos de texto, es decir, para convertir las cadenas originales en secuencias enteras donde cada entero representa el índice de una palabra en un vocabulario.\n",
        "\n",
        "Se utiliza un esquema de estandarización de cadenas personalizado (tira de caracteres de puntuación excepto < y >) y el esquema de división predeterminado (dividir en espacios en blanco)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRP-cE_h15Vc"
      },
      "source": [
        "def custom_standarization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "\n",
        "strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
        "strip_chars = strip_chars.replace(\"<\", \"\")\n",
        "strip_chars = strip_chars.replace(\">\", \"\")\n",
        "\n",
        "vectorization = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=SEQ_LENGTH,\n",
        "    standardize=custom_standarization,\n",
        ")\n",
        "vectorization.adapt(text_data)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRykzCMF2F-a"
      },
      "source": [
        "### Construyendo una tf.data.Dataset canalización para la capacitación\n",
        "\n",
        "Generaremos pares de imágenes y leyendas correspondientes usando un tf.data.Dataset objeto. La canalización consta de dos pasos:\n",
        "\n",
        "1.  Leer la imagen del disco\n",
        "2.  Tokeniza los cinco subtítulos correspondientes a la imagen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZOIP03X2LuM"
      },
      "source": [
        "def read_image(img_path, size=IMAGE_SIZE):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SIZE)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    return img\n",
        "\n",
        "\n",
        "def make_dataset(images, captions):\n",
        "    img_dataset = tf.data.Dataset.from_tensor_slices(images).map(\n",
        "        read_image, num_parallel_calls=AUTOTUNE\n",
        "    )\n",
        "    cap_dataset = tf.data.Dataset.from_tensor_slices(captions).map(\n",
        "        vectorization, num_parallel_calls=AUTOTUNE\n",
        "    )\n",
        "    dataset = tf.data.Dataset.zip((img_dataset, cap_dataset))\n",
        "    dataset = dataset.batch(BATCH_SIZE).shuffle(256).prefetch(AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Pasar la lista de imágenes y la lista de leyendas\n",
        "train_dataset = make_dataset(list(train_data.keys()), list(train_data.values()))\n",
        "valid_dataset = make_dataset(list(valid_data.keys()), list(valid_data.values()))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twllZvvc2fSE"
      },
      "source": [
        "### Construyendo el modelo\n",
        "\n",
        "La arquitectura de subtítulos de imágenes consta de tres modelos:\n",
        "\n",
        "*   Una CNN: utilizada para extraer las características de la imagen.\n",
        "*   Un TransformerEncoder: las características de la imagen extraída luego se pasan a un Transformer Codificador que genera una nueva representación de entradas.\n",
        "*   Un TransformerDecoder: este modelo toma la salida del codificador y los datos de texto (secuencial) como entradas e intentos de aprender a generar subtítulos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV4-V-l221Dm"
      },
      "source": [
        "def get_cnn_model():\n",
        "    base_model = efficientnet.EfficientNetB0(\n",
        "        input_shape=(*IMAGE_SIZE, 3), include_top=False, weights=\"imagenet\",\n",
        "    )\n",
        "    # Congela en extractor de funciones\n",
        "    base_model.trainable = False\n",
        "    base_model_out = base_model.output\n",
        "    base_model_out = layers.Reshape((-1, 1280))(base_model_out)\n",
        "    cnn_model = keras.models.Model(base_model.input, base_model_out)\n",
        "    return cnn_model\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = layers.Dense(embed_dim, activation=\"relu\")\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, training, mask=None):\n",
        "        inputs = self.dense_proj(inputs)\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=None\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        return proj_input\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class TransformerDecoderBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ff_dim = ff_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim)]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "\n",
        "        self.embedding = PositionalEmbedding(\n",
        "            embed_dim=EMBED_DIM, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE\n",
        "        )\n",
        "        self.out = layers.Dense(VOCAB_SIZE)\n",
        "        self.dropout_1 = layers.Dropout(0.1)\n",
        "        self.dropout_2 = layers.Dropout(0.5)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, training, mask=None):\n",
        "        inputs = self.embedding(inputs)\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        inputs = self.dropout_1(inputs, training=training)\n",
        "\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
        "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
        "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=combined_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        proj_out = self.layernorm_3(out_2 + proj_output)\n",
        "        proj_out = self.dropout_2(proj_out, training=training)\n",
        "\n",
        "        preds = self.out(proj_out)\n",
        "        return preds\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "class ImageCaptioningModel(keras.Model):\n",
        "    def __init__(\n",
        "        self, cnn_model, encoder, decoder, num_captions_per_image=5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.cnn_model = cnn_model\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n",
        "        self.num_captions_per_image = num_captions_per_image\n",
        "\n",
        "    def calculate_loss(self, y_true, y_pred, mask):\n",
        "        loss = self.loss(y_true, y_pred)\n",
        "        mask = tf.cast(mask, dtype=loss.dtype)\n",
        "        loss *= mask\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
        "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
        "        accuracy = tf.math.logical_and(mask, accuracy)\n",
        "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
        "        mask = tf.cast(mask, dtype=tf.float32)\n",
        "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
        "\n",
        "    def _compute_loss_and_acc(self, batch_data, training=True):\n",
        "        batch_img, batch_seq = batch_data\n",
        "        batch_loss = 0\n",
        "        batch_acc = 0\n",
        "\n",
        "        # 1. Obtenga instrucciones de imágenes\n",
        "        img_embed = self.cnn_model(batch_img)\n",
        "\n",
        "        # 2. Pasar cada uno de los cinco subtítulos al decodificador\n",
        "        #    junto con la salidas del codificador y calcular la pérdida y precisión para cada título\n",
        "        \n",
        "        for i in range(self.num_captions_per_image):\n",
        "            with tf.GradientTape() as tape:\n",
        "                # 3. Pasar las incrustaciones de imágenes al codificador\n",
        "                encoder_out = self.encoder(img_embed, training=training)\n",
        "\n",
        "                batch_seq_inp = batch_seq[:, i, :-1]\n",
        "                batch_seq_true = batch_seq[:, i, 1:]\n",
        "\n",
        "                # 4. Calcular la máscara para la secuencia de entrada\n",
        "                mask = tf.math.not_equal(batch_seq_inp, 0)\n",
        "\n",
        "                # 5. Pase las salidas del codificador,\n",
        "                # las entradas de secuencia alojadas con máscaras al codificador\n",
        "                batch_seq_pred = self.decoder(\n",
        "                    batch_seq_inp, encoder_out, training=training, mask=mask\n",
        "                )\n",
        "\n",
        "                # 6. Calcular la pérdida y la precisión\n",
        "                loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n",
        "                acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n",
        "\n",
        "                # 7. Actualizar la pérdida y la precisión del lote\n",
        "                batch_loss += loss\n",
        "                batch_acc += acc\n",
        "\n",
        "            # 8. Obtener la lista de entrenamiento\n",
        "            train_vars = (\n",
        "                self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "            )\n",
        "\n",
        "            # 9. Obtener los degradados\n",
        "            grads = tape.gradient(loss, train_vars)\n",
        "\n",
        "            # 10. Actualizar el entrenamiento\n",
        "            self.optimizer.apply_gradients(zip(grads, train_vars))\n",
        "\n",
        "        return batch_loss, batch_acc / float(self.num_captions_per_image)\n",
        "\n",
        "    def train_step(self, batch_data):\n",
        "        loss, acc = self._compute_loss_and_acc(batch_data)\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.acc_tracker.update_state(acc)\n",
        "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "\n",
        "    def test_step(self, batch_data):\n",
        "        loss, acc = self._compute_loss_and_acc(batch_data, training=False)\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.acc_tracker.update_state(acc)\n",
        "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker, self.acc_tracker]\n",
        "\n",
        "\n",
        "cnn_model = get_cnn_model()\n",
        "encoder = TransformerEncoderBlock(\n",
        "    embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=NUM_HEADS\n",
        ")\n",
        "decoder = TransformerDecoderBlock(\n",
        "    embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=NUM_HEADS\n",
        ")\n",
        "caption_model = ImageCaptioningModel(\n",
        "    cnn_model=cnn_model, encoder=encoder, decoder=decoder\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R9710WQ3ilb"
      },
      "source": [
        "### Entrenamiento de modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9KSpbNr3jok",
        "outputId": "e2c91051-3a16-4708-a247-593a67db9ba6"
      },
      "source": [
        "# Definir la función de perdida\n",
        "cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction=\"none\"\n",
        ")\n",
        "\n",
        "# Criterios de parada anticipada\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "# Compilar el Modelo\n",
        "caption_model.compile(optimizer=keras.optimizers.Adam(), loss=cross_entropy)\n",
        "\n",
        "# Ajustar el modelo\n",
        "caption_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=valid_dataset,\n",
        "    callbacks=[early_stopping],\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "809/809 [==============================] - 194s 217ms/step - loss: 19.4160 - acc: 0.2227 - val_loss: 14.9976 - val_acc: 0.3776\n",
            "Epoch 2/5\n",
            "809/809 [==============================] - 172s 206ms/step - loss: 17.0314 - acc: 0.3171 - val_loss: 14.3587 - val_acc: 0.3903\n",
            "Epoch 3/5\n",
            "809/809 [==============================] - 172s 206ms/step - loss: 16.5030 - acc: 0.3282 - val_loss: 14.1016 - val_acc: 0.3963\n",
            "Epoch 4/5\n",
            "809/809 [==============================] - 173s 208ms/step - loss: 16.3149 - acc: 0.3323 - val_loss: 13.9662 - val_acc: 0.3978\n",
            "Epoch 5/5\n",
            "809/809 [==============================] - 174s 209ms/step - loss: 16.2256 - acc: 0.3361 - val_loss: 13.8544 - val_acc: 0.4004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f68dc00a690>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}